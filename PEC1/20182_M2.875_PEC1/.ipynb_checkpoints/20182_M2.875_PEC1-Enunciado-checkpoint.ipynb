{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.875 · Deep Learning · PEC1</p>\n",
    "<p style=\"margin: 0; text-align:right;\">2018-2 · Máster universitario en Ciencia de datos (Data science)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "\n",
    "# PEC 1: Redes neuronales completamente conectadas\n",
    "\n",
    "En esta práctica implementaremos una red neuronal completamente conectada de dos formas diferentes: \n",
    "\n",
    "<ol start=\"1\">\n",
    "  <li>Partiendo de cero utilizando únicamente la librería numpy</li>\n",
    "  <li>Utilizando la librería Keras y TensorFlow</li>\n",
    "</ol>\n",
    "\n",
    "Posteriormente utilizaremos las dos implementaciones para entrenar dos redes neuronales iguales en un conjunto de datos y compararemos el rendimiento.\n",
    "\n",
    "**Importante: Cada uno de los ejercicios puede suponer varios minutos de ejecución, por lo que la entrega debe hacerse en formato notebook y en formato html donde se vea el código y los resultados y comentarios de cada ejercicio. Para exportar el notebook a html puede hacerse desde el menú File $\\to$ Download as $\\to$ HTML.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Carga de datos\n",
    "\n",
    "El siguiente código carga los paquetes necesarios para la práctica y además lee los datos que utilizaremos para entrenar la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open(\"data.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "features = data[\"features\"]\n",
    "labels = data[\"labels\"]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(features, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Redes neuronales utilizando numpy \n",
    "\n",
    "A continuación implementaremos todas las funciones necesarias para entrenar una red neuronal completamente conectada utilizando únicamente la librería numpy. El objetivo es poder entrenar una red neuronal con cualquier número de capas en la cual la última capa tendrá una única neurona con función de activación sigmoid y las demás capas cualquier número de neuronas con función de activación relu.\n",
    "\n",
    "La siguiente figura muestra un diagrama de como implementaremos el proceso de entrenamiento de la red neuronal:\n",
    "\n",
    "<img src=\"diag.png\" alt=\"Diagrama del entrenamiento de la red neuronal\" style=\"height: 550px;\"/>\n",
    "\n",
    "El desarrollo está estructurado en funciones básicas que se componen según el siguiente esquema:\n",
    "\n",
    "- L_layer_model\n",
    "  - initialize_parameters\n",
    "  - L_model_forward\n",
    "    - linear_activation_forward\n",
    "      - linear_forward\n",
    "      - sigmoid\n",
    "      - relu\n",
    "  - compute_cost\n",
    "  - L_model_backward\n",
    "    - linear_activation_backward\n",
    "      - linear_backward\n",
    "      - sigmoid_backward\n",
    "      - relu_backward\n",
    "  - update_parameters\n",
    "- accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notación**:\n",
    "- Denotamos $L$ el número de capas de la red neuronal.\n",
    "- La matriz de pesos que conecta una capa con la siguiente la denotamos con la letra $W$, mientras que el vector de bias lo denotamos con la letra $b$.\n",
    "- Superíndice $[l]$ denota una cantidad asociada con la capa número $l$. \n",
    "    - Ejemplo: $a^{[L]}$ denota la salida de la capa número $L$.\n",
    "    - Ejemplo: Las variables $W^{[L]}$ y $b^{[L]}$ denotan la matriz de pesos y el vector de bias que conectan la capa $L-1$ con la capa $L$ respectivamente.\n",
    "- Superíndice $(i)$ denota una cantidad asociada con el ejemplo $i$-ésimo. \n",
    "    - Ejemplo: $x^{(i)}$ es el ejemplo del conjunto de entrenamiento número $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Inicialización de parámetros\n",
    "\n",
    "El primer paso para entrenar una red neuronal consiste en inicializar de forma aleatoria los parámetros del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Inicializar las matrices de parámetros y los vectores de bias. Las matrices de pesos se deben inicializar utilizando la distribución normal y los vectores de bias se deben inicializar con ceros. Para las matrices de pesos podéis utilizar 0.1*np.random.randn(shape) indicando el tamaño correcto.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    layer_dims -- lista que contiene las dimensiones de cada capa de la red\n",
    "    \n",
    "    Devuelve:\n",
    "    parameters -- diccionario que contiene los parametros \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- matriz de pesos de tamaño (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- vector de bias de tamaño (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = None\n",
    "        parameters['b' + str(l)] = None\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Propagación hacia delante\n",
    "\n",
    "En una capa concreta de una red neuronal, las entradas de las neuronas se combinan de forma lineal antes de pasar por la función de activación según la siguiente fórmula:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Calcular la combinación lineal de las entradas a una capa de la red neuronal.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implementa la parte lineal de la propagación hacia delante de una capa\n",
    "\n",
    "    Argumentos:\n",
    "    A -- salida de la capa anterior (o datos de entrada): (número de neuronas de la capa anterior, número de ejemplos)\n",
    "    W -- matriz de pesos: (número de neuronas de la capa actual, número de neuronas de la capa anterior)\n",
    "    b -- vector de bias: (número de neuronas de la capa actual, 1)\n",
    "\n",
    "    Devuelve:\n",
    "    Z -- la entrada a la función de activación\n",
    "    cache -- una tripleta que contiene \"A\", \"W\" y \"b\", utilizada después para la propagación hacia atrás\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = None\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez se ha calculado la combinación lineal de las entradas de una capa se debe aplicar una función de activación no lineal antes de enviar las salidas a la siguiente capa. Si denotamos $g$ la función de activación (en nuestro caso relu o sigmoid), tenemos la siguiente fórmula:\n",
    "\n",
    "$$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} + b^{[l]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación definimos las funciones de activación que utilizaremos en la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Calcular la combinación lineal de las entradas utilizando la función implementada anteriormente y aplicar la función de activación no lineal que corresponda.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia delante de una capa incluyendo la función de activación\n",
    "\n",
    "    Argumentos:\n",
    "    A_prev -- salida de la capa anterior (o datos de entrada): \n",
    "                (número de neuronas de la capa anterior, número de ejemplos)\n",
    "    W -- matriz de pesos: (número de neuronas de la capa actual, número de neuronas de la capa anterior)\n",
    "    b -- vector de bias: (número de neuronas de la capa actual, 1)\n",
    "    activation -- el nombre de la función de activación a utilizar en la capa: \"sigmoid\" o \"relu\"\n",
    "\n",
    "    Devuelve:\n",
    "    A -- la salida de la capa después de aplicar la función de activación\n",
    "    cache -- una dupla que contiene \"linear_cache\" y \"activation_cache\", utilizada después para la propagación hacia atrás\n",
    "    \"\"\"\n",
    "    \n",
    "    Z, linear_cache = None\n",
    "    \n",
    "    if activation == \"sigmoid\":    \n",
    "        A, activation_cache = None\n",
    "    elif activation == \"relu\":\n",
    "        A, activation_cache = None\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dados los datos de entrada, la salida de la red neuronal se calcula aplicando diferentes capas una detrás de otra. Si denotamos la última capa como $L$, la salida de la red neuronal se corresponde con la salida de la última capa $A^{[L]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Calcular la salida de la red neuronal aplicando $L-1$ capas con función de activación relu y una última capa con función de activación sigmoid.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia delante de la red neuronal completa\n",
    "    \n",
    "    Argumentos:\n",
    "    X -- datos: matriz de tamaño (número de variables, número de ejemplos)\n",
    "    parameters -- salida de la función initialize_parameters()\n",
    "    \n",
    "    Devuelve:\n",
    "    AL -- salida de la red neuronal\n",
    "    caches -- lista de caches que contiene todas las caches de la función linear_activation_forward(), las caches\n",
    "                indexadas de 0 a L-2 corresponden a las caches de la función de activación relu y la cache indexada\n",
    "                como L-1 corresponde a la cache de la función de activación sigmoid\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    # Implementa primero las L-1 capas con función de activación relu\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = None\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implementa la última capa con función de activación sigmoid\n",
    "    AL, cache = None\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Función de coste\n",
    "\n",
    "Una vez hemos obtenido la salida de la red neuronal podemos obtener un valor que mida el rendimiento de la red neuronal utilizando una función de coste $\\mathcal{L}$. En nuestro caso utilizaremos la función de coste log-loss, que viene definida por la siguiente fórmula:\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Calcular el valor de la función de coste log-loss dada la salida de la red neuronal junto con las etiquetas correctas.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Calcula la función de coste\n",
    "\n",
    "    Argumentos:\n",
    "    AL -- vector que contiene la salida de la red, corresponde a las probabilidades que predice la red neuronal\n",
    "            para cada ejemplo: (1, número de ejemplos)\n",
    "    Y -- vector con las etiquetas correctas para los datos de entrada a la red: (1, número de ejemplos)\n",
    "\n",
    "    Devuelve:\n",
    "    cost -- valor de la función de coste log-loss\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    cost = None\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Propagación hacia atrás\n",
    "\n",
    "Para entrenar una red neuronal es necesario calcular el gradiente de la función de coste repescto a los parámetros de la red, para lo cual utilizaremos la propagación hacia atrás. La propagación hacia atrás consiste en aplicar la regla de la cadena para calcular el gradiente de la función de coste paso a paso en cada capa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aplicar la regla de la cadena en la parte lineal de la neurona, supongamos que ya hemos calculado la derivada $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. Entonces, para calcular las derivadas $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ podemos utilizar las siguientes fórmulas:\n",
    "\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Calcular las derivadas de la parte lineal para una sola capa.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implementa la parte lineal de la propagación hacia atrás para una única capa\n",
    "\n",
    "    Argumentos:\n",
    "    dZ -- derivada de la función de coste con respecto a la salida lineal de la capa actual\n",
    "    cache -- tripleta que contiene los valores (A_prev, W, b), provinientes de la función linear_forward\n",
    "\n",
    "    Devuelve:\n",
    "    dA_prev -- derivada de la función de coste con respecto a la salida de la capa anterior (l-1): \n",
    "                tiene el mismo tamaño que A_prev\n",
    "    dW -- derivada de la función de coste con respecto a la matriz de pesos W de la capa actual (l):\n",
    "                tiene el mismo tamaño que W\n",
    "    db -- derivada de la función de coste con respecto al vector de bias b de la capa actual (l):\n",
    "                tiene el mismo tamaño que b\n",
    "    \"\"\"\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = None\n",
    "    db = None\n",
    "    dA_prev = None\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso consiste en aplicar la regla de la cadena a la parte no lineal de las neuronas, es decir, a las funciones de activación. Para esto, si denotamos $g$ la función de activación, podemos utilizar la siguiente fórmula:\n",
    "\n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$\n",
    "\n",
    "Donde $*$ indica el producto componente a componente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación calculamos las derivadas de las funciones de activación que utilizamos en la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "    return dZ\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Combinar el cálculo de la derivada de las funciones de activación con la derivada de la parte lineal para obtener, a partir de la derivada de la función de coste respecto la activación de una capa, la derivada de la función de coste respecto a los parámetros de la capa y respecto a las activaciones de la capa anterior.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia atrás de una única capa incluyendo la función de activación\n",
    "    \n",
    "    Argumentos:\n",
    "    dA -- derivada de la función de coste con respecto a la salida de la capa actual (l)\n",
    "    cache -- dupla que contiene \"linear_cache\" y \"activation_cache\", provinientes de la función linear_activation_forward\n",
    "    activation -- el nombre de la función de activación utilizada en la capa actual (l): \"sigmoid\" o \"relu\"\n",
    "    \n",
    "    Devuelve:\n",
    "    dA_prev -- derivada de la función de coste con respecto a la salida de la capa anterior (l-1):\n",
    "                tiene el mismo tamaño que A_prev\n",
    "    dW -- derivada de la función de coste con respecto a la matriz de pesos W de la capa actual (l):\n",
    "                tiene el mismo tamaño que W\n",
    "    db -- derivada de la función de coste con respecto al vector de bias b de la capa actual (l):\n",
    "                tiene el mismo tamaño que b\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = None\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = None\n",
    "        \n",
    "    dA_prev, dW, db = None\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, es posible calcular la derivada de la función de coste respecto a cualquiera de los parámetros aplicando las funciones recién implementadas empezando por la última capa. Observemos que para inicializar la propagación hacia atrás es necesario calcular primero el valor de $\\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Aplicar la propagación hacia atrás para calcular el gradiente de la función de coste. Observad que el valor de $\\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$ viene calculado en la variable dAL y que la última capa tiene función de activación sigmoid mientras que todas las demás tienen función de activación relu.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia atrás de la red neuronal completa\n",
    "    \n",
    "    Argumentos:\n",
    "    AL -- salida de la red neuronal, proviene de la función L_model_forward\n",
    "    Y -- vector con las etiquetas correctas para cada ejemplo del conjunto de datos: (1, número de ejemplos)\n",
    "    caches -- lista de caches que contiene todas las caches de la función linear_activation_forward(), las caches\n",
    "                indexadas de 0 a L-2 corresponden a las caches de la función de activación relu y la cache indexada\n",
    "                como L-1 corresponde a la cache de la función de activación sigmoid\n",
    "    \n",
    "    Devuelve:\n",
    "    grads -- Un diccionario con las derivadas de la función de coste respecto de cada variable:\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # Inicialización de la propagación hacia atrás\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Gradiente de la última capa\n",
    "    current_cache = None\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = current_cache\n",
    "\n",
    "    # Gradiente de las capas restantes\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = None\n",
    "        dA_prev_temp, dW_temp, db_temp = current_cache\n",
    "        grads[\"dA\" + str(l + 1)] = None\n",
    "        grads[\"dW\" + str(l + 1)] = None\n",
    "        grads[\"db\" + str(l + 1)] = None\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Actualización de parámetros\n",
    "\n",
    "Una vez disponemos del gradiente de la función de coste podemos utilizar el método del descenso del gradiente para actualizar los parámetros de la red neuronal. Si denotamos $\\alpha$ la velocidad de aprendizaje, las fórmulas para aplicar un paso del descenso del gradiente son:\n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Actualizar los parámetros de la red neuronal aplicando un paso del descenso del gradiente.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Actualiza los parámetros utilizando el descenso del gradiente\n",
    "    \n",
    "    Argumentos:\n",
    "    parameters -- diccionario que contiene los parámetros de la red neuronal\n",
    "    grads -- diccionario con las derivadas de la función de coste respecto a cada parámetro,\n",
    "                corresponde a la salida de la función L_model_backward\n",
    "    \n",
    "    Devuelve:\n",
    "    parameters -- diccionario con los parámetros actualizados:\n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = None\n",
    "        parameters[\"b\" + str(l+1)] = None\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con todo esto es posible entrenar la red neuronal combinando las funciones definidas anteriormente para aplicar diversas iteraciones del descenso del gradiente e ir actualizando los parámetros de la red de forma reiterada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código muestra cómo entrenar la red neuronal que hemos construido utilizando únicamente la librería numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate, num_iterations, print_cost):\n",
    "    \"\"\"\n",
    "    Implementa una red neuronal de L capas donde las L-1 primeras capas tienen función de activación relu y \n",
    "    la última capa tiene función de activación sigmoid.\n",
    "    \n",
    "    Argumentos:\n",
    "    X -- datos: matriz de tamaño (número de variables, número de ejemplos)\n",
    "    Y -- vector con las etiquetas correctas para cada ejemplo del conjunto de datos: (1, número de ejemplos)\n",
    "    layers_dims -- lista de longitud (número de capas + 1) que contiene el número de variables y el número \n",
    "                    de neuronas en cada capa, \n",
    "    learning_rate -- velocidad de aprendizaje para aplicar el método del descenso del gradiente\n",
    "    num_iterations -- número de pasos para aplicar el descenso del gradiente\n",
    "    print_cost -- si el valor es True, escribe el valor de la función de coste cada 10 iteraciones\n",
    "    \n",
    "    Devuelve:\n",
    "    parameters -- parámetros ajustados de la red neuronal\n",
    "    \"\"\"\n",
    "    \n",
    "    # Inicialización de los parámetros\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        # Propagación hacia delante\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Cálculo de la función de coste\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Propagación hacia atrás\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Actualización de parámetros\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Escribe el valor de la función de coste cada 10 iteraciones\n",
    "        if print_cost and i % 10 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Redes neuronales utilizando Keras\n",
    "\n",
    "A continuación definiremos una red neuronal completamente conectada igual a la que hemos implementado anteriormente pero esta vez utilizando la librería Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Definir una red neuronal completamente conectada a partir de una lista que contiene el número de neuronas que debe tener cada capa de la red. Las primeras capas deben tener función de activación relu y la última capa debe tener función de activación sigmoid. Todas ellas tienen que tener kernel_initializer=\"random_normal\" y bias_initializer=\"zeros\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def keras_model(layers_dims, learning_rate):\n",
    "    \"\"\"\n",
    "    Crea, utilizando Keras, una red neuronal de L capas completamente conectadas donde las L-1 primeras capas\n",
    "    tienen función de activación relu y la última capa tiene función de activación sigmoid.\n",
    "    \n",
    "    Argumentos:\n",
    "    layers_dims -- lista de longitud (número de capas + 1) que contiene el número de variables y el número \n",
    "                    de neuronas en cada capa, \n",
    "    learning_rate -- velocidad de aprendizaje para aplicar el método del descenso del gradiente\n",
    "    \n",
    "    Devuelve:\n",
    "    modelo -- objeto de Keras que representa la red neuronal\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(layers_dims)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Añadir L-1 capas con función de activación relu y una última capa con función de activación sigmoid,\n",
    "    # cada capa debe tener el número de neuronas indicado en la variable layers_dims, el tamaño de la capa\n",
    "    # de entrada viene dado en layers_dims[0]\n",
    "    \n",
    "    model.compile(optimizer=SGD(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento de la red neuronal\n",
    "\n",
    "Con todas las funciones implementadas anteriormente es posible entrenar una red neuronal completamente conectada con cualquier número de capas y cualquier número de neuronas en cada capa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación definimos la estructura de capas que tendrá la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [100, 20, 5, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar la red neuronal que hemos construido únicamente utilizando numpy debemos ejecutar el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = L_layer_model(train_x.T, train_y.reshape(1, -1), layers_dims=layers_dims, learning_rate=0.1, \n",
    "                           num_iterations=250, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar la red neuronal que hemos construido utilizando Keras debemos ejecutar el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras_model(layers_dims = layers_dims, learning_rate = 0.1)\n",
    "model.fit(train_x, train_y, epochs=250, batch_size=train_x.shape[0], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, podemos utilizar la siguiente función para calcular la precisión que obtenemos con la red neuronal construida utilizando únicamente numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, y, parameters):\n",
    "    \"\"\"\n",
    "    Calcula la precisión de las predicciones de la red neuronal.\n",
    "    \n",
    "    Argumentos:\n",
    "    X -- datos: matriz de tamaño (número de variables, número de ejemplos)\n",
    "    parameters -- parámetros de la red neuronal entrenada\n",
    "    \n",
    "    Returns:\n",
    "    accuracy -- valor entre 0 y 1 que representa la precisión de la red neuronal\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Propagación hacia delante\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    # Conversión de la salida de la red a valores 0 o 1\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0, i] > 0.5:\n",
    "            p[0, i] = 1\n",
    "        else:\n",
    "            p[0, i] = 0\n",
    "            \n",
    "    accuracy = np.sum((p == y)) / m\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra la precisión obtenida tanto con la red construida con numpy como con la red construida con Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Red construida con numpy\")\n",
    "print(\"Precisión {:.2f}\".format(accuracy(test_x.T, test_y.reshape(1, -1), parameters)))\n",
    "print(\"---\")\n",
    "print(\"Red construida con Keras\")\n",
    "print(\"Precisión {:.2f}\".format(model.evaluate(test_x, test_y, verbose=0)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código permite calcular el tiempo que tarda cada red neuronal en entrenarse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "parameters = L_layer_model(train_x.T, train_y.reshape(1, -1), layers_dims=layers_dims, \n",
    "                           learning_rate=0.1, num_iterations=250, print_cost=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "model = keras_model(layers_dims=layers_dims, learning_rate=0.1)\n",
    "model.fit(train_x, train_y, epochs=250, batch_size=train_x.shape[0], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Análisis:</strong> Comparar el rendimiento, tanto en tiempo de ejecución como en precisión, de las dos implementaciones de la red neuronal. Utilizar diferentes hiperparámetros en la comparación: probar con diferentes valores para las dimensiones de las capas, diferente número de capas, número de iteraciones, etc. ¿Qué factores pueden estar creando las diferencias observadas?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "Parte del codigo utilizado para desarrollar esta práctica proviene del curso de Coursera [\"Neural networks and deep learning\"](https://www.coursera.org/learn/neural-networks-deep-learning?specialization=deep-learning)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
